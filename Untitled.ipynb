{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b514513-ded6-452f-9986-bc5dea86305b",
   "metadata": {},
   "source": [
    "print(f\"Script begins the run\")\n",
    "import argparse\n",
    "import logging\n",
    "import logging.config\n",
    "import os\n",
    "import sys\n",
    "from argparse import ArgumentParser\n",
    "\n",
    "import torch\n",
    "from omegaconf import OmegaConf\n",
    "\n",
    "from lib import config_utils, data_utils, utils\n",
    "from lib.formatter import RawFormatter\n",
    "from lib.logger import prepare_logger\n",
    "\n",
    "print(f\"Packages read\")\n",
    "\n",
    "parser = ArgumentParser(\n",
    "    description='U-TILISE: A Sequence-to-sequence Model for Cloud Removal in Optical Satellite Time Series (Training)',\n",
    "    formatter_class=RawFormatter\n",
    ")\n",
    "parser.add_argument(\n",
    "    'config_file', type=str,\n",
    "    default=\"./configs/config_sen12mscrts_train.yaml\",\n",
    "    help='yaml configuration file to augment/overwrite the settings in configs/config_sen12mscrts_train.yaml'\n",
    ")\n",
    "parser.add_argument(\n",
    "    '--save_dir', type=str, \n",
    "    default=\"./results\",\n",
    "    help='Path to the directory where models and logs should be saved'\n",
    ")\n",
    "parser.add_argument('--dataset_name', default=\"allclear\", type=str, help='Use Weights & Biases instead of TensorBoard')\n",
    "parser.add_argument('--wandb', action='store_true', default=False, help='Use Weights & Biases instead of TensorBoard')\n",
    "parser.add_argument('--wandb_project', type=str, default='utilise', help='Wandb project name')\n",
    "\n",
    "args, _ = parser.parse_known_args()\n",
    "\n",
    "prog_name = 'U-TILISE: A Sequence-to-sequence Model for Cloud Removal in Optical Satellite Time Series (Training)'\n",
    "print('\\n{}\\n{}\\n'.format(prog_name, '=' * len(prog_name)))\n",
    "\n",
    "if not os.path.exists(args.config_file):\n",
    "    raise FileNotFoundError(f'ERROR: Cannot find the yaml configuration file: {args.config_file}')\n",
    "\n",
    "# Import the user configuration file\n",
    "cfg_custom = config_utils.read_config(args.config_file)\n",
    "\n",
    "if not cfg_custom:\n",
    "    sys.exit(1)\n",
    "\n",
    "# Augment/overwrite the default parameter settings with the runtime arguments given by the user\n",
    "cfg_default = config_utils.read_config('configs/default_sen12.yaml')\n",
    "config = OmegaConf.merge(cfg_default, cfg_custom)\n",
    "config.output.output_directory = args.save_dir\n",
    "\n",
    "if args.wandb:\n",
    "    config.wandb = OmegaConf.create()\n",
    "    config.wandb.project = args.wandb_project\n",
    "\n",
    "# Create the output directory. The name of the output directory is a combination of the current date, time, and an\n",
    "# optional suffix.\n",
    "config.output.experiment_folder = utils.create_output_directory(config)\n",
    "\n",
    "# Set up the logger\n",
    "log_file = os.path.join(config.output.experiment_folder, 'run.log') if config.output.experiment_folder else None\n",
    "logger = prepare_logger('root_logger', level=logging.INFO, log_to_console=True, log_file=log_file)\n",
    "\n",
    "# Print runtime arguments to the console\n",
    "logger.info('Configuration file: %s', args.config_file)\n",
    "logger.info('\\nSettings\\n--------\\n')\n",
    "config_utils.print_config(config, logger=logger)\n",
    "\n",
    "if config.misc.random_seed is not None:\n",
    "    utils.set_seed(config.misc.random_seed)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37adffb8-f8c8-466e-89aa-5d9a2c0b0ae8",
   "metadata": {},
   "source": [
    "# Default data and model settings (i.e., settings used during training)\n",
    "if args.dataset_name == \"allclear\":\n",
    "\n",
    "    from dataset_wrapper import get_loader\n",
    "    train_loader = get_loader(config)\n",
    "    val_loader = get_loader(config)\n",
    "\n",
    "elif args.dataset_name == \"original\":\n",
    "    config_file_train = 'configs/demo.yaml'\n",
    "    config_file_test = 'configs/config_earthnet2021_test_simulation.yaml'\n",
    "    def get_dataloader_testdata(\n",
    "        config_file_train: str,\n",
    "        config_file_test: str,\n",
    "        run_mode: str = 'test'\n",
    "    ) -> torch.utils.data.dataloader.DataLoader:\n",
    "        if not os.path.isfile(config_file_train):\n",
    "            raise FileNotFoundError(f'Cannot find the configuration file used during training: {config_file_train}\\n')\n",
    "    \n",
    "        if not os.path.isfile(config_file_test):\n",
    "            raise FileNotFoundError(f'Cannot find the test configuration file: {config_file_test}\\n')\n",
    "    \n",
    "        # Read the configuration file used during training\n",
    "        config = config_utils.read_config(config_file_train)\n",
    "    \n",
    "        # Merge generic data settings (used during training) with test-specific data settings\n",
    "        config_testdata = config_utils.read_config(config_file_test)\n",
    "        config.data.update(config_testdata.data)\n",
    "        if 'mask' in config_testdata:\n",
    "            config.mask.update(config_testdata.mask)\n",
    "        config.misc.run_mode = run_mode\n",
    "    \n",
    "        # Get the data loader\n",
    "        dset = data_utils.get_dataset(config, phase=run_mode)\n",
    "        dataloader = torch.utils.data.DataLoader(\n",
    "            dataset=dset, batch_size=1, shuffle=False, num_workers=8, drop_last=False\n",
    "        )\n",
    "        \n",
    "        return dataloader\n",
    "    train_loader = get_dataloader_testdata(config_file_train, config_file_test)\n",
    "    val_loader = get_dataloader_testdata(config_file_train, config_file_test)\n",
    "\n",
    "# ------------------------------------------------- Data loaders ------------------------------------------------- #\n",
    "# logger.info('\\nInitialize data loader (training set)...')\n",
    "# train_loader = data_utils.get_dataloader(\n",
    "#     config, phase='train', pin_memory=config.misc.pin_memory, drop_last=True, logger=logger\n",
    "# )\n",
    "# logger.info('Initialize data loader (validation set)...\\n')\n",
    "# val_loader = data_utils.get_dataloader(\n",
    "#     config, phase='val', pin_memory=config.misc.pin_memory, drop_last=False, logger=logger\n",
    "# )\n",
    "\n",
    "logger.info('Number of training samples: %d', train_loader.dataset.__len__())\n",
    "logger.info('Number of validation samples: %d', val_loader.dataset.__len__())\n",
    "logger.info('Variable sequence lengths: %r\\n', train_loader.dataset.variable_seq_length)\n",
    "\n",
    "# ----------------------------------------- Prepare the output directory ----------------------------------------- #\n",
    "logger.info('\\nPrepare output folders and files\\n--------------------------------\\n')\n",
    "\n",
    "# Save the path of the checkpoint directory\n",
    "config.output.checkpoint_dir = os.path.join(config.output.experiment_folder, 'checkpoints')\n",
    "os.makedirs(config.output.checkpoint_dir, exist_ok=True)\n",
    "logger.info('Model weights will be stored in: %s\\n', config.output.checkpoint_dir)\n",
    "\n",
    "# Write the runtime configuration to file\n",
    "config_file = os.path.join(config.output.experiment_folder, 'config.yaml')\n",
    "config_utils.write_config(config, config_file)\n",
    "\n",
    "# ----------------------------------------------- Define the model ----------------------------------------------- #\n",
    "logger.info('\\nModel Architecture\\n------------------\\n')\n",
    "logger.info('Architecture: %s', config.method.model_type)\n",
    "\n",
    "input_dim = train_loader.dataset.num_channels\n",
    "model, args_model = utils.get_model(config, input_dim, logger)\n",
    "logger.info('Number of trainable parameters: %d\\n', utils.count_model_parameters(model))\n",
    "\n",
    "# Log model parameters to file\n",
    "config_file = os.path.join(config.output.experiment_folder, 'model_config.yaml')\n",
    "config_utils.write_config(OmegaConf.create({config.method.model_type: args_model}), config_file)\n",
    "\n",
    "# Write model architecture to txt file\n",
    "if config.output.plot_model_txt:\n",
    "    file = os.path.join(config.output.experiment_folder, 'model_parameters.txt')\n",
    "    logger.info('Writing model architecture to file: %s\\n', file)\n",
    "    utils.write_model_structure_to_file(\n",
    "        file, model, config.training_settings.batch_size, train_loader.dataset.seq_length, input_dim,\n",
    "        train_loader.dataset.image_size\n",
    "    )\n",
    "\n",
    "# --------------------------------------------------- Training --------------------------------------------------- #\n",
    "logger.info('\\nPrepare training\\n----------------\\n')\n",
    "logger.info('Python version: %s', sys.version)\n",
    "logger.info('Torch version: %s', torch.__version__)\n",
    "logger.info('CUDA version: %s\\n', torch.version.cuda)\n",
    "\n",
    "# Get optimizer and learning rate scheduler\n",
    "optimizer = utils.get_optimizer(config, model, logger)\n",
    "scheduler = utils.get_scheduler(config, optimizer, logger)\n",
    "\n",
    "if config.misc.random_seed is not None:\n",
    "    utils.set_seed(config.misc.random_seed)\n",
    "\n",
    "# Initialize the trainer and start training\n",
    "trainer = utils.get_trainer(config, train_loader, val_loader, model, optimizer, scheduler)\n",
    "trainer.train()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a9288ed-d114-4f64-b87d-638afebbb1aa",
   "metadata": {},
   "source": [],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e854178-d47b-4063-9616-a207979ed730",
   "metadata": {},
   "source": [],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6246a06d-327a-446f-8773-d43575dd9c87",
   "metadata": {},
   "source": [],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de90c867-8119-473a-8f66-aa3486d979f5",
   "metadata": {},
   "source": [
    "for batch in train_loader: break"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cba59f2b-d470-442b-8cec-ca16ec9bd6f3",
   "metadata": {},
   "source": [
    "print(f\"keys: {batch.keys()}\")\n",
    "print(f\"\"\"x: {batch[\"x\"].shape}\"\"\")\n",
    "print(f\"\"\"y: {batch[\"y\"].shape}\"\"\")\n",
    "print(f\"\"\"masks: {batch[\"masks\"].shape}\"\"\")\n",
    "print(f\"\"\"position_days: {batch[\"position_days\"]}\"\"\")\n",
    "print(f\"\"\"days: {batch[\"days\"]}\"\"\")\n",
    "print(f\"\"\"sample_index: {batch[\"sample_index\"]}\"\"\")\n",
    "print(f\"\"\"c_index_rgb: {batch[\"c_index_rgb\"]}\"\"\")\n",
    "print(f\"\"\"c_index_nir: {batch[\"c_index_nir\"]}\"\"\")\n",
    "print(f\"\"\"cloud_mask: {batch[\"cloud_mask\"].shape}\"\"\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9778a8a1-f89c-4124-b1ad-f54d75a0a120",
   "metadata": {},
   "source": [
    "import matplotlib.pyplot as plt"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21f51569-667c-4ffa-b3f5-acec94745aaf",
   "metadata": {},
   "source": [
    "x = batch[\"x\"][0][2].permute(1,2,0)\n",
    "x = x * 5\n",
    "x = torch.clip(x, 0, 1)\n",
    "plt.figure(), plt.imshow(x[:,:,(2,1,0)])\n",
    "\n",
    "x = batch[\"masks\"][0][2].permute(1,2,0)\n",
    "x = x * 5\n",
    "x = torch.clip(x, 0, 1)\n",
    "plt.figure(), plt.imshow(x[:,:,0])\n",
    "\n",
    "x = batch[\"cloud_mask\"][0][2].permute(1,2,0)\n",
    "x = x * 5\n",
    "x = torch.clip(x, 0, 1)\n",
    "plt.figure(), plt.imshow(x[:,:,0])"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b7f3432-23bd-4281-a954-78896222cde9",
   "metadata": {},
   "source": [],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baf7a9a5-f236-42c8-9519-cfe13d74f032",
   "metadata": {},
   "source": [
    "import sys, os\n",
    "if \"ck696\" in os.getcwd():\n",
    "    sys.path.append(\"/share/hariharan/ck696/allclear\")\n",
    "else:\n",
    "    sys.path.append(\"/share/hariharan/cloud_removal/allclear\")\n",
    "\n",
    "from dataset.dataloader_v1 import CRDataset\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "class CRDatasetWrapper(Dataset):\n",
    "    def __init__(self, original_dataset):\n",
    "        self.original_dataset = original_dataset\n",
    "\n",
    "    def __len__(self):\n",
    "        return 2376\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        batch = self.original_dataset[idx]\n",
    "        # cond_image = batch[\"input_images\"][:4,...].reshape(9,256,256) * 2 - 1\n",
    "        # gt_image = batch[\"target\"][:4,...].reshape(3,256,256) * 2 - 1\n",
    "        \n",
    "        return {\"x\": batch[\"input_images\"].permute(1,0,2,3), \n",
    "                \"y\": batch[\"target\"].permute(1,0,2,3),\n",
    "                \"masks\": batch[\"input_cld_shdw\"].permute(1,0,2,3).max(dim=1, keepdim=True).values,\n",
    "                \"position_days\": batch[\"time_differences\"],\n",
    "                \"days\": batch[\"time_differences\"],\n",
    "                \"sample_index\": 0,\n",
    "                \"c_index_rgb\": torch.Tensor([0,1,2]),\n",
    "                \"c_index_nir\": torch.Tensor([3]),\n",
    "                \"cloud_mask\": batch[\"target_cld_shdw\"].permute(1,0,2,3).max(dim=1, keepdim=True).values\n",
    "               }\n",
    "\n",
    "import json\n",
    "# with open('/share/hariharan/cloud_removal/metadata/v3/s2s_tx6_v1.json') as f:\n",
    "#     metadata = json.load(f)\n",
    "    \n",
    "# for i in range(len(metadata)):\n",
    "#     for j in range(6):\n",
    "#         # metadata[f\"{i}\"][\"target\"][0][1] = \"/share/hariharan/cloud_removal/MultiSensor/dataset_30k_v4/\" + metadata[f\"{i}\"][\"target\"][0][1].split(\"dataset_30k_v4\")[1]\n",
    "#         metadata[f\"{i}\"][\"s2_toa\"][j][1] = \"/share/hariharan/cloud_removal/MultiSensor/dataset_30k_v4/\" + metadata[f\"{i}\"][\"s2_toa\"][j][1].split(\"dataset_30k_v4\")[1]\n",
    "#     try:\n",
    "#         for j in range(6):\n",
    "#             # metadata[f\"{i}\"][\"target\"][0][1] = \"/share/hariharan/cloud_removal/MultiSensor/dataset_30k_v4/\" + metadata[f\"{i}\"][\"target\"][0][1].split(\"dataset_30k_v4\")[1]\n",
    "#             metadata[f\"{i}\"][\"s1\"][j][1] = \"/share/hariharan/cloud_removal/MultiSensor/dataset_30k_v4/\" + metadata[f\"{i}\"][\"s1\"][j][1].split(\"dataset_30k_v4\")[1]\n",
    "#     except:\n",
    "#         pass\n",
    "# save metadata to json\n",
    "# file_name = '/share/hariharan/cloud_removal/metadata/v3/s2s_tx6_v1_bh.json'\n",
    "# with open(file_name, 'w') as f:\n",
    "#     json.dump(metadata, f)\n",
    "\n",
    "import json\n",
    "with open('/share/hariharan/cloud_removal/metadata/v3/s2s_tx6_v1_bh.json') as f:\n",
    "    metadata = json.load(f)\n",
    "    \n",
    "clds_shdws = torch.ones(1000, 2, 256, 256)\n",
    "\n",
    "train_data = CRDataset(metadata, \n",
    "                    selected_rois=\"all\", \n",
    "                    main_sensor=\"s2_toa\", \n",
    "                    aux_sensors=[],\n",
    "                    aux_data=[\"cld_shdw\"],\n",
    "                    format=\"stp\",\n",
    "                    target=\"s2s\",\n",
    "                    clds_shdws=clds_shdws,\n",
    "                    tx=6,\n",
    "                    s2_toa_channels=[4,3,2,8]\n",
    "                    )\n",
    "# wrapped_train_data = CRDatasetWrapper(train_data)\n",
    "# phase_loader = DataLoader(train_data, batch_size=2, shuffle=True, num_workers=2, pin_memory=True)\n",
    "\n",
    "wrapped_train_data = CRDatasetWrapper(train_data)\n",
    "phase_loader = DataLoader(wrapped_train_data, batch_size=2, shuffle=True, num_workers=2, pin_memory=True)\n",
    "for ac_batch in phase_loader: break"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faf3c096-5182-4d72-a83e-ef2edc40fa0b",
   "metadata": {},
   "source": [
    "for ac_batch in phase_loader: break"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeef276f-b8f6-4e4e-85a7-11eb21e5466b",
   "metadata": {},
   "source": [
    "for key in ac_batch.keys():\n",
    "    if len(ac_batch[key].shape) == 1:\n",
    "        print(f\"{key}: {ac_batch[key]}\")\n",
    "    else:\n",
    "        print(f\"{key}: {ac_batch[key].shape}\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c9ab813-3ff7-4868-a6d3-a5e9ba68deca",
   "metadata": {},
   "source": [
    "x = ac_batch[\"x\"][0][5].permute(1,2,0)\n",
    "x = x * 5\n",
    "x = torch.clip(x, 0, 1)\n",
    "plt.figure(), plt.imshow(x[:,:,:3])"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b53c4f2-2115-44ab-a05c-0ca5a3ee7856",
   "metadata": {},
   "source": [
    "trainer = utils.get_trainer(config, phase_loader, phase_loader, model, optimizer, scheduler)\n",
    "trainer.train()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "744d9bae-2a9f-47ea-b78a-45b2e061916c",
   "metadata": {},
   "source": [],
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "allclear",
   "language": "python",
   "name": "allclear"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
